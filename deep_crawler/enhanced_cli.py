#!/usr/bin/env python3
"""
Enhanced CLI with LangChain/LangGraph integration.
Provides both traditional and advanced workflow options.
"""

import toml
import requests
import hashlib
import asyncio
import textwrap
from pathlib import Path
from tqdm import tqdm
from deep_crawler.crawler.firecrawl_async import crawl_urls
from deep_crawler.indexing import faiss_store
from deep_crawler.crawler.extractor import simple_extract
from deep_crawler.llm.verifier import dangling_citations

# Enhanced LLM imports
try:
    from deep_crawler.llm.enhanced_planner import plan_with_sections
    from deep_crawler.llm.enhanced_summariser import summarise_section, generate_research_insights
    # Skip LangGraph for now to avoid complexity
    # from deep_crawler.llm.langgraph_workflow import research_workflow
    LANGCHAIN_AVAILABLE = True
    LANGGRAPH_AVAILABLE = False
    print("ðŸš€ LangChain Enhanced Mode: Available")
except ImportError as e:
    LANGCHAIN_AVAILABLE = False
    LANGGRAPH_AVAILABLE = False
    print(f"âš ï¸ LangChain Enhanced Mode: Unavailable ({e})")
    # Fallback to traditional imports
    from deep_crawler.llm import planner, summariser

# Load config from root directory
CFG = toml.load(Path(__file__).parent.parent / "config.toml")

def searx(q, n):
    """Search using SearX with enhanced error handling."""
    try:
        r = requests.get(
            CFG["search"]["searx_url"],
            params={"q": q, "format": "json", "language": "en"},
            timeout=15,
        ).json()
        return [hit["url"] for hit in r.get("results", [])[:n]]
    except Exception as e:
        print(f"âš ï¸ Search error for '{q}': {e}")
        return []

def enhanced_main(question: str, use_langgraph: bool = False) -> str:
    """
    Enhanced main function with LangChain integration.
    
    Args:
        question: Research question
        use_langgraph: Whether to use the full LangGraph workflow (disabled for now)
        
    Returns:
        str: Generated research report
    """
    # Skip LangGraph for now
    if LANGCHAIN_AVAILABLE:
        return run_enhanced_traditional(question)
    else:
        return run_traditional_workflow(question)

def run_langgraph_workflow(question: str) -> str:
    """Run the full LangGraph workflow."""
    print(f"ðŸš€ LangGraph Workflow: Starting advanced research orchestration")
    print(f"ðŸ” Research Question: {question}")
    
    try:
        # Use the LangGraph workflow
        report = research_workflow.run_research(question)
        
        # Save the report
        out = Path(f"report_{hashlib.md5(question.encode()).hexdigest()[:8]}.md")
        out.write_text(report, encoding="utf-8")
        
        print(f"ðŸŽ‰ LangGraph Research Complete!")
        print(f"ðŸ“„ Report saved to: {out}")
        print(f"ðŸ“Š Report length: {len(report):,} characters")
        
        return report
        
    except Exception as e:
        print(f"âŒ LangGraph workflow failed: {e}")
        print(f"ðŸ”„ Falling back to enhanced traditional workflow...")
        return run_enhanced_traditional(question)

def run_enhanced_traditional(question: str) -> str:
    """Run enhanced traditional workflow with LangChain components."""
    print(f"ðŸš€ Enhanced Traditional Workflow: LangChain-powered research")
    print(f"ðŸ” Researching: {question}")
    
    try:
        # Enhanced planning
        print(f"ðŸ¤– Enhanced AI Planner: Creating strategic research plan...")
        outline, keywords, sections = plan_with_sections(question)
        
        print(f"ðŸ“‹ Research Plan: {len(keywords)} keywords, {len(sections)} sections")
        print(f"ðŸŽ¯ Strategic Keywords: {', '.join(keywords[:5])}{'...' if len(keywords) > 5 else ''}")
        
        # Enhanced outline display
        print(f"\nðŸ“‹ Research Outline:")
        for line in outline.splitlines()[:8]:
            if line.strip() and line.startswith('#'):
                print(f"   {line}")
        if len(outline.splitlines()) > 8:
            print(f"   ... and more sections")
        print("")

        # Enhanced source searching
        print(f"ðŸ” Enhanced Source Discovery:")
        urls = []
        for i, kw in enumerate(keywords, 1):
            print(f"   ðŸ”Ž [{i}/{len(keywords)}] Searching: '{kw}'")
            new_urls = searx(kw, CFG["search"]["urls_per_keyword"])
            urls += new_urls
            print(f"      âž¡ï¸ Found {len(new_urls)} URLs for '{kw}'")
        
        urls = list(dict.fromkeys(urls))  # dedupe
        print(f"ðŸŒ Total unique URLs discovered: {len(urls)}")

        # Enhanced crawling
        print(f"ðŸ•·ï¸ Enhanced Web Crawling:")
        for i, url in enumerate(urls[:10], 1):  # Show first 10 URLs
            print(f"   ðŸŒ [{i}/{len(urls)}] {url[:80]}{'...' if len(url) > 80 else ''}")
        
        if len(urls) > 10:
            print(f"   ðŸŒ ... and {len(urls) - 10} more URLs")
        
        pages = asyncio.run(crawl_urls(urls))
        if not pages:
            raise RuntimeError("No pages scraped.")

        print(f"\nâœ… Crawling Results:")
        print(f"   ðŸ“„ Successfully crawled: {len(pages)} pages")
        print(f"   âŒ Failed to crawl: {len(urls) - len(pages)} pages")
        print(f"   ðŸ“Š Success rate: {len(pages)/len(urls)*100:.1f}%")
        
        # Enhanced indexing
        print(f"\nðŸ”— Enhanced Knowledge Processing:")
        texts = [p["markdown"][:8192] for p in pages]
        print(f"   ðŸ“Š Processing {len(texts)} documents...")
        print(f"   ðŸ§  Building semantic search index...")
        index = faiss_store.build(texts)
        print(f"   âœ… Knowledge base built with {len(texts)} documents")

        # Enhanced content generation
        print(f"\nâœï¸ Enhanced AI Content Generation:")
        doc = [f"# {question}", ""]
        
        for i, sec in enumerate(sections, 1):
            print(f"\nðŸ“ Section {i}/{len(sections)}: {sec}")
            
            # Use enhanced summarizer
            section_content = summarise_section(sec, index, texts)
            
            doc.append(f"## {sec}")
            doc.append(section_content)
            doc.append("")
            doc.append("")

        # Generate enhanced insights
        if len(sections) > 1:
            print(f"\nðŸ§  Generating Research Insights...")
            all_content = [doc[i] for i in range(len(doc)) if doc[i].strip() and not doc[i].startswith('#')]
            insights = generate_research_insights(question, all_content[:5])  # Limit for token management
            
            if insights and len(insights) > 100:
                doc.append("## Key Insights and Analysis")
                doc.append(insights)
                doc.append("")
                doc.append("")

        # Enhanced references
        print(f"\nðŸ“š Enhanced Reference Processing:")
        doc.append("## References")
        doc.append("")
        print(f"   ðŸ“‘ Processing {len(pages)} source references...")
        
        for i, p in enumerate(pages, 1):
            title = p["title"] or "Untitled"
            ref = f"[{i}] **{title}** â€” {p['url']}"
            doc.append(ref)
            
            if i <= 5:
                print(f"   ðŸ“– [{i}] {title[:50]}{'...' if len(title) > 50 else ''}")
        
        if len(pages) > 5:
            print(f"   ðŸ“š ... and {len(pages) - 5} more references")

        md = "\n".join(doc)
        
        # Enhanced verification
        print(f"\nðŸ” Enhanced Quality Verification:")
        dangling = dangling_citations(md, set(range(1, len(pages) + 1)))
        if dangling:
            print(f"âš ï¸ Citation Issues: {len(dangling)} dangling citations found")
            print(f"   ðŸ”§ Citations needing review: {', '.join(map(str, dangling))}")
        else:
            print(f"   âœ… All citations properly referenced")

        # Save and report
        out = Path(f"report_{hashlib.md5(question.encode()).hexdigest()[:8]}.md")
        out.write_text(md, encoding="utf-8")
        
        print(f"\nðŸŽ‰ Enhanced Research Complete!")
        print(f"ðŸ“„ Report saved to: {out}")
        print(f"ðŸ“Š Enhanced Statistics:")
        print(f"   â€¢ Total characters: {len(md):,}")
        print(f"   â€¢ Sections generated: {len(sections)}")
        print(f"   â€¢ Sources analyzed: {len(pages)}")
        print(f"   â€¢ Keywords researched: {len(keywords)}")
        print(f"   â€¢ URLs processed: {len(urls)}")
        print(f"   â€¢ Success rate: {len(pages)}/{len(urls)} ({len(pages)/len(urls)*100:.1f}%)")
        
        return md
        
    except Exception as e:
        print(f"âŒ Enhanced workflow failed: {e}")
        print(f"ðŸ”„ Falling back to traditional workflow...")
        return run_traditional_workflow(question)

def run_traditional_workflow(question: str) -> str:
    """Fallback to traditional workflow."""
    print(f"ðŸ”„ Traditional Workflow: Basic research mode")
    print(f"ðŸ” Researching: {question}")
    
    # Use original planner and summariser
    outline, kws = planner.plan(question)
    print(f"ðŸ“‹ Research Plan: {len(kws)} keywords, {len([l for l in outline.splitlines() if l.startswith('##')])} sections")
    print("ðŸ” Keywords:", ", ".join(kws))

    urls = []
    for kw in kws:
        urls += searx(kw, CFG["search"]["urls_per_keyword"])
    urls = list(dict.fromkeys(urls))
    print(f"ðŸŒ Found {len(urls)} unique URLs to research")

    pages = asyncio.run(crawl_urls(urls))
    if not pages:
        raise RuntimeError("No pages scraped.")

    print(f"ðŸ“„ Successfully crawled {len(pages)} pages")
    
    texts = [p["markdown"][:8192] for p in pages]
    index = faiss_store.build(texts)
    print(f"ðŸ”— Built search index with {len(texts)} documents")

    doc = [f"# {question}", ""]
    
    sections = []
    for line in outline.splitlines():
        if line.startswith("## "):
            section_title = line.strip("# ").strip()
            sections.append(section_title)
    
    print(f"âœï¸ Writing {len(sections)} sections...")
    
    for i, sec in enumerate(sections, 1):
        print(f"ðŸ“ Section {i}/{len(sections)}: {sec}")
        section_content = summariser.summarise_section(sec, index, texts)
        print(f"   âœ… Generated {len(section_content)} characters")
        
        doc.append(f"## {sec}")
        doc.append(section_content)
        doc.append("")
        doc.append("")

    print("ðŸ“š Adding references...")
    doc.append("## References")
    doc.append("")
    for i, p in enumerate(pages, 1):
        title = p["title"] or "Untitled"
        ref = f"[{i}] **{title}** â€” {p['url']}"
        doc.append(ref)

    md = "\n".join(doc)
    dangling = dangling_citations(md, set(range(1, len(pages) + 1)))
    if dangling:
        print(f"âš ï¸ Found {len(dangling)} dangling citations: {dangling}")

    out = Path(f"report_{hashlib.md5(question.encode()).hexdigest()[:8]}.md")
    out.write_text(md, encoding="utf-8")
    print(f"âœ… Complete! Report saved to {out}")
    print(f"ðŸ“Š Final report: {len(md)} characters, {len(sections)} sections, {len(pages)} sources")
    
    return md

def main(question: str) -> str:
    """
    Main research function with automatic workflow selection.
    
    Args:
        question: The research question
        
    Returns:
        str: Generated research report
    """
    # Check if we should use advanced workflows
    use_langgraph = CFG.get("llm", {}).get("use_langgraph", False)
    
    if LANGCHAIN_AVAILABLE:
        print(f"ðŸš€ LangChain Enhanced Mode: Active")
        if use_langgraph:
            print(f"ðŸ”¬ Using LangGraph advanced workflow orchestration")
        else:
            print(f"ðŸ”§ Using LangChain enhanced traditional workflow")
    else:
        print(f"ðŸ”„ Traditional Mode: LangChain not available")
    
    return enhanced_main(question, use_langgraph=use_langgraph)

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print("Usage: enhanced_cli.py \"research question\"")
        sys.exit(1)
    main(" ".join(sys.argv[1:]))
